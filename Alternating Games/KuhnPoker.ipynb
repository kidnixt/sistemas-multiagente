{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.kuhn.kuhn import KuhnPoker\n",
    "from agents.counterfactual_regret import CounterFactualRegret\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = KuhnPoker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_classes = [ CounterFactualRegret, CounterFactualRegret ]\n",
    "my_agents = {}\n",
    "g.reset()\n",
    "for i, agent in enumerate(g.agents):\n",
    "    my_agents[agent] = agent_classes[i](game=g, agent=agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent agent_0\n",
      "Agent agent_0 policies:\n",
      "OrderedDict([('0', array([1.49772346e-05, 9.99985023e-01])), ('0b', array([1.50652325e-05, 9.99984935e-01])), ('0p', array([0.00224472, 0.99775528])), ('0pb', array([0.5, 0.5])), ('1', array([1.51080224e-05, 9.99984892e-01])), ('1b', array([9.99984959e-01, 1.50407605e-05])), ('1p', array([0.00221099, 0.99778901])), ('1pb', array([0.5, 0.5])), ('2', array([0.99636091, 0.00363909])), ('2b', array([9.99985105e-01, 1.48951382e-05])), ('2p', array([9.99985105e-01, 1.48951382e-05])), ('2pb', array([7.48525087e-06, 9.99992515e-01]))])\n",
      "\n",
      "Training agent agent_1\n",
      "Agent agent_1 policies:\n",
      "OrderedDict([('0', array([1.50852316e-05, 9.99984915e-01])), ('0b', array([0.5, 0.5])), ('0p', array([1.49790294e-05, 9.99985021e-01])), ('0pb', array([0.5, 0.5])), ('1', array([9.99984990e-01, 1.50096061e-05])), ('1b', array([1.50294577e-05, 9.99984971e-01])), ('1p', array([1.50294577e-05, 9.99984971e-01])), ('1pb', array([9.99992495e-01, 7.50491572e-06])), ('2', array([9.99985094e-01, 1.49062398e-05])), ('2b', array([1.49916047e-05, 9.99985008e-01])), ('2p', array([1.49916047e-05, 9.99985008e-01])), ('2pb', array([9.99992547e-01, 7.45323098e-06]))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for agent in g.agents:\n",
    "    print('Training agent ' + agent)\n",
    "    my_agents[agent].train(100000)\n",
    "    print('Agent ' + agent + ' policies:')\n",
    "    print(OrderedDict(map(lambda n: (n, my_agents[agent].node_dict[n].policy()), sorted(my_agents[agent].node_dict.keys()))))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards: {'agent_0': -0.1195, 'agent_1': 0.1195}\n"
     ]
    }
   ],
   "source": [
    "cum_rewards = dict(map(lambda agent: (agent, 0.), g.agents))\n",
    "niter = 2000\n",
    "for _ in range(niter):\n",
    "    g.reset()\n",
    "    turn = 0\n",
    "    while not g.done():\n",
    "        #print('Turn: ', turn)\n",
    "        #print('\\tPlayer: ', g.agent_selection)\n",
    "        #print('\\tObservation: ', g.observe(g.agent_selection))\n",
    "        a = my_agents[g.agent_selection].action()\n",
    "        #print('\\tAction: ', g._moves[a])\n",
    "        g.step(action=a)\n",
    "        turn += 1\n",
    "    #print('Rewards: ', g.rewards)\n",
    "    for agent in g.agents:\n",
    "        cum_rewards[agent] += g.rewards[agent]\n",
    "print('Average rewards:', dict(map(lambda agent: (agent, cum_rewards[agent]/niter), g.agents)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check learned policies against theoretical policies:\n"
     ]
    }
   ],
   "source": [
    "print('Check learned policies against theoretical policies:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: J_ - History: [] - Probability of betting: 0.9999850227653966\n"
     ]
    }
   ],
   "source": [
    "JX_b = my_agents[g.agents[0]].node_dict['0'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: J_ - History: [] - Probability of betting: {JX_b}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: Q_ - History: pb - Probability of betting: 0.5 - Theoretic value: 1.3333183560987298 -  Difference: 0.8333183560987298\n"
     ]
    }
   ],
   "source": [
    "QX_pb_b = my_agents[g.agents[0]].node_dict['1pb'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: Q_ - History: pb - Probability of betting: {QX_pb_b} - Theoretic value: {JX_b+1/3} -  Difference: {abs(QX_pb_b - (JX_b+1/3))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: K_ - History: [] - Probability of betting: 0.0036390860847953827 - Theoretic value: 2.99995506829619 -  Difference: 2.9963159822113945\n"
     ]
    }
   ],
   "source": [
    "KX_b = my_agents[g.agents[0]].node_dict['2'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: K_ - History: [] - Probability of betting: {KX_b} - Theoretic value: {3 * JX_b} -  Difference: {abs(KX_b - 3 * JX_b)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: _J - History: p - Probability of betting: 0.997755280363976 - Theoretic value: 0.3333333333333333 -  Difference: 0.6644219470306427\n"
     ]
    }
   ],
   "source": [
    "XJ_p_b = my_agents[g.agents[0]].node_dict['0p'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: _J - History: p - Probability of betting: {XJ_p_b} - Theoretic value: {1/3} -  Difference: {abs(XJ_p_b - 1/3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: 0 - Hand: _Q - History: b - Probability of betting: 1.5040760460848901e-05 - Theoretic value: 0.3333333333333333 -  Difference: 0.3333182925728725\n"
     ]
    }
   ],
   "source": [
    "XQ_b_b = my_agents[g.agents[0]].node_dict['1b'].policy()[1]\n",
    "print(f'Agent: 0 - Hand: _Q - History: b - Probability of betting: {XQ_b_b} - Theoretic value: {1/3} -  Difference: {abs(XQ_b_b - 1/3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pettingzoo_games",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
